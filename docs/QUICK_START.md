# å¿«é€Ÿå¼€å§‹æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œåœ¨å‡ åˆ†é’Ÿå†…å®Œæˆç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ã€‚

## å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- pip æˆ– conda

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd machine_learning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨conda
conda env create -f environment.yml
conda activate ml-framework
```

## 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

```python
import pandas as pd
from src.data import DataLoader, DataPreprocessor

# åŠ è½½ç¤ºä¾‹æ•°æ®ï¼ˆæˆ–ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®ï¼‰
loader = DataLoader()

# ä»CSVæ–‡ä»¶åŠ è½½
data = loader.load_from_file('examples/data/sample_data.csv')

# æˆ–åˆ›å»ºç¤ºä¾‹æ•°æ®
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
data['target'] = y

print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
print(data.head())
```

### 2. æ•°æ®é¢„å¤„ç†

```python
# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = DataPreprocessor()

# æ·»åŠ é¢„å¤„ç†æ­¥éª¤
preprocessor.add_step('handle_missing', method='mean')
preprocessor.add_step('normalize', method='standard')

# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
X = data.drop('target', axis=1)
y = data['target']

# é¢„å¤„ç†æ•°æ®
X_processed = preprocessor.fit_transform(X)

print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {X_processed.shape}")
```

### 3. è®­ç»ƒæ¨¡å‹

```python
from src.training import ModelTrainer
from sklearn.model_selection import train_test_split

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
trainer = ModelTrainer(
    model_type='random_forest',
    hyperparameters={'n_estimators': 100, 'random_state': 42}
)

model = trainer.train(X_train, y_train)
print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
```

### 4. è¯„ä¼°æ¨¡å‹

```python
from src.training import ModelValidator

# è¯„ä¼°æ¨¡å‹
validator = ModelValidator()
metrics = validator.evaluate(
    model, X_test, y_test,
    metrics=['accuracy', 'precision', 'recall', 'f1']
)

print("æ¨¡å‹è¯„ä¼°ç»“æœ:")
for metric, value in metrics.items():
    print(f"{metric}: {value:.4f}")
```

### 5. ä¿å­˜å’Œéƒ¨ç½²æ¨¡å‹

```python
from src.deployment import ModelSerializer, ModelInferenceEngine

# ä¿å­˜æ¨¡å‹
serializer = ModelSerializer()
serializer.save_model(
    model, 
    'my_first_model.joblib',
    format='joblib',
    metadata={'accuracy': metrics['accuracy']}
)

# åˆ›å»ºæ¨ç†å¼•æ“
engine = ModelInferenceEngine('my_first_model.joblib')

# è¿›è¡Œé¢„æµ‹
sample_data = X_test[:5]
predictions = engine.predict(sample_data)
print(f"é¢„æµ‹ç»“æœ: {predictions}")
```

ğŸ‰ **æ­å–œï¼** æ‚¨å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ï¼

## å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: åˆ†ç±»ä»»åŠ¡

```python
# å®Œæ•´çš„åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹
from src.data import DataLoader, DataPreprocessor
from src.training import ModelTrainer, ModelValidator
from src.deployment import ModelSerializer

def classification_pipeline(data_path, target_column):
    # 1. åŠ è½½æ•°æ®
    loader = DataLoader()
    data = loader.load_from_file(data_path)
    
    # 2. é¢„å¤„ç†
    preprocessor = DataPreprocessor()
    preprocessor.add_step('handle_missing', method='mode')
    preprocessor.add_step('encode_categorical', method='onehot')
    preprocessor.add_step('normalize', method='standard')
    
    X = data.drop(target_column, axis=1)
    y = data[target_column]
    X_processed = preprocessor.fit_transform(X)
    
    # 3. åˆ†å‰²æ•°æ®
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, test_size=0.2, stratify=y, random_state=42
    )
    
    # 4. è®­ç»ƒæ¨¡å‹
    trainer = ModelTrainer('random_forest', {'n_estimators': 100})
    model = trainer.train(X_train, y_train)
    
    # 5. è¯„ä¼°
    validator = ModelValidator()
    metrics = validator.evaluate(model, X_test, y_test)
    
    # 6. ä¿å­˜
    serializer = ModelSerializer()
    serializer.save_model(model, 'classification_model.joblib')
    
    return model, metrics

# ä½¿ç”¨ç¤ºä¾‹
# model, results = classification_pipeline('data.csv', 'target')
```

### åœºæ™¯2: å›å½’ä»»åŠ¡

```python
def regression_pipeline(data_path, target_column):
    # ç±»ä¼¼åˆ†ç±»ä»»åŠ¡ï¼Œä½†ä½¿ç”¨å›å½’æ¨¡å‹å’ŒæŒ‡æ ‡
    loader = DataLoader()
    data = loader.load_from_file(data_path)
    
    preprocessor = DataPreprocessor()
    preprocessor.add_step('handle_missing', method='mean')
    preprocessor.add_step('remove_outliers', method='iqr')
    preprocessor.add_step('normalize', method='standard')
    
    X = data.drop(target_column, axis=1)
    y = data[target_column]
    X_processed = preprocessor.fit_transform(X)
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, test_size=0.2, random_state=42
    )
    
    # ä½¿ç”¨å›å½’æ¨¡å‹
    trainer = ModelTrainer('linear_regression')
    model = trainer.train(X_train, y_train)
    
    # ä½¿ç”¨å›å½’æŒ‡æ ‡
    validator = ModelValidator()
    metrics = validator.evaluate(
        model, X_test, y_test,
        metrics=['mse', 'rmse', 'mae', 'r2']
    )
    
    return model, metrics
```

### åœºæ™¯3: è¶…å‚æ•°ä¼˜åŒ–

```python
from src.training import HyperparameterOptimizer

def optimized_training(X_train, y_train):
    # å®šä¹‰æœç´¢ç©ºé—´
    search_space = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10]
    }
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = HyperparameterOptimizer(
        model_type='random_forest',
        optimization_method='grid_search'
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    best_params, best_score, history = optimizer.optimize(
        X_train, y_train,
        search_space=search_space,
        cv_folds=5,
        scoring='accuracy'
    )
    
    print(f"æœ€ä½³å‚æ•°: {best_params}")
    print(f"æœ€ä½³å¾—åˆ†: {best_score}")
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    trainer = ModelTrainer('random_forest', best_params)
    final_model = trainer.train(X_train, y_train)
    
    return final_model, best_params
```

## ç›‘æ§å’Œéƒ¨ç½²

### æ·»åŠ ç›‘æ§

```python
from src.monitoring import PerformanceMonitor, AlertManager

# åˆ›å»ºæ€§èƒ½ç›‘æ§
monitor = PerformanceMonitor(
    monitoring_interval=1.0,
    enable_system_monitoring=True
)

# åˆ›å»ºå‘Šè­¦ç®¡ç†å™¨
alert_manager = AlertManager()
alert_manager.add_rule(
    rule_id='high_latency',
    metric_name='inference_latency',
    threshold=1000,
    comparison='greater_than',
    severity='warning',
    description='æ¨ç†å»¶è¿Ÿè¿‡é«˜'
)

# å¯åŠ¨ç›‘æ§
monitor.start_monitoring()

# åœ¨æ¨ç†æ—¶è®°å½•æŒ‡æ ‡
@monitor.track_inference
def predict_with_monitoring(data):
    return engine.predict(data)
```

### åˆ›å»ºAPIæœåŠ¡

```python
from flask import Flask, request, jsonify
from src.deployment import ModelInferenceEngine

app = Flask(__name__)
engine = ModelInferenceEngine('my_first_model.joblib')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # è·å–è¾“å…¥æ•°æ®
        data = request.json
        input_df = pd.DataFrame([data])
        
        # è¿›è¡Œé¢„æµ‹
        prediction = engine.predict(input_df)
        
        return jsonify({
            'prediction': prediction.tolist(),
            'status': 'success'
        })
    
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 400

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

## ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†åŸºç¡€ç”¨æ³•ï¼Œå¯ä»¥æ¢ç´¢æ›´é«˜çº§çš„åŠŸèƒ½ï¼š

### 1. æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹

```bash
# è¿è¡ŒåŸºç¡€ç¤ºä¾‹
python examples/basic_usage.py

# è¿è¡Œé«˜çº§è®­ç»ƒç¤ºä¾‹
python examples/advanced_training.py

# è¿è¡Œéƒ¨ç½²ç¤ºä¾‹
python examples/deployment_example.py

# è¿è¡Œç›‘æ§ç¤ºä¾‹
python examples/monitoring_example.py
```

### 2. é˜…è¯»è¯¦ç»†æ–‡æ¡£

- [APIå‚è€ƒæ–‡æ¡£](API_REFERENCE.md) - å®Œæ•´çš„APIè¯´æ˜
- [æœ€ä½³å®è·µæŒ‡å—](BEST_PRACTICES.md) - ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ
- [éƒ¨ç½²æŒ‡å—](DEPLOYMENT.md) - è¯¦ç»†çš„éƒ¨ç½²è¯´æ˜

### 3. è‡ªå®šä¹‰é…ç½®

åˆ›å»ºæ‚¨è‡ªå·±çš„é…ç½®æ–‡ä»¶ï¼š

```yaml
# config.yaml
data:
  default_format: "csv"
  cache_enabled: true
  
training:
  default_cv_folds: 5
  random_state: 42
  
deployment:
  model_format: "joblib"
  enable_caching: true
  
monitoring:
  log_level: "INFO"
  metrics_retention_days: 30
```

### 4. é›†æˆåˆ°ç°æœ‰é¡¹ç›®

```python
# åœ¨ç°æœ‰é¡¹ç›®ä¸­ä½¿ç”¨æ¡†æ¶
from src import MLFramework

# åˆ›å»ºæ¡†æ¶å®ä¾‹
ml = MLFramework(config_path='config.yaml')

# ä½¿ç”¨æ¡†æ¶åŠŸèƒ½
data = ml.load_data('data.csv')
model = ml.train_model(data, 'target', model_type='random_forest')
ml.deploy_model(model, 'production')
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†å¤§æ•°æ®é›†ï¼Ÿ

A: ä½¿ç”¨åˆ†å—å¤„ç†å’Œå¹¶è¡Œè®¡ç®—ï¼š

```python
# åˆ†å—å¤„ç†å¤§æ–‡ä»¶
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    processed_chunk = preprocessor.transform(chunk)
    # å¤„ç†æ¯ä¸ªå—
```

### Q: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹ï¼Ÿ

A: æ‰©å±•ModelTrainerç±»ï¼š

```python
from src.training import ModelTrainer

class CustomModelTrainer(ModelTrainer):
    def _create_model(self):
        if self.model_type == 'my_custom_model':
            return MyCustomModel(**self.hyperparameters)
        return super()._create_model()
```

### Q: å¦‚ä½•é›†æˆå¤–éƒ¨æ•°æ®æºï¼Ÿ

A: æ‰©å±•DataLoaderç±»ï¼š

```python
from src.data import DataLoader

class CustomDataLoader(DataLoader):
    def load_from_custom_source(self, connection_params):
        # å®ç°è‡ªå®šä¹‰æ•°æ®æºåŠ è½½é€»è¾‘
        pass
```

## è·å–å¸®åŠ©

- æŸ¥çœ‹ [examples/](../examples/) ç›®å½•ä¸­çš„å®Œæ•´ç¤ºä¾‹
- é˜…è¯» [docs/](.) ç›®å½•ä¸­çš„è¯¦ç»†æ–‡æ¡£
- æ£€æŸ¥ [tests/](../tests/) ç›®å½•ä¸­çš„æµ‹è¯•ç”¨ä¾‹

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸš€