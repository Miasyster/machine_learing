"""
æ—¶é—´åºåˆ—è®­ç»ƒç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ—¶é—´åºåˆ—è®­ç»ƒå™¨è¿›è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†
2. æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†
3. å¤šæ¨¡å‹è®­ç»ƒå’Œæ¯”è¾ƒ
4. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
5. Walk-forwardéªŒè¯
6. ç»“æœåˆ†æå’Œå¯è§†åŒ–
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

# å¯¼å…¥æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å—
from src.training.time_series_trainer import TimeSeriesTrainer, TimeSeriesTrainingConfig
from src.training.time_series_validation import TimeSeriesConfig


def generate_time_series_data(n_samples: int = 1000, 
                            n_features: int = 5,
                            trend: bool = True,
                            seasonality: bool = True,
                            noise_level: float = 0.1) -> Tuple[pd.DataFrame, pd.Series, pd.DatetimeIndex]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„æ—¶é—´åºåˆ—æ•°æ®
    
    Args:
        n_samples: æ ·æœ¬æ•°é‡
        n_features: ç‰¹å¾æ•°é‡
        trend: æ˜¯å¦åŒ…å«è¶‹åŠ¿
        seasonality: æ˜¯å¦åŒ…å«å­£èŠ‚æ€§
        noise_level: å™ªå£°æ°´å¹³
        
    Returns:
        ç‰¹å¾æ•°æ®, ç›®æ ‡å˜é‡, æ—¶é—´ç´¢å¼•
    """
    # åˆ›å»ºæ—¶é—´ç´¢å¼•
    start_date = datetime(2020, 1, 1)
    time_index = pd.date_range(start=start_date, periods=n_samples, freq='D')
    
    # ç”ŸæˆåŸºç¡€ç‰¹å¾
    np.random.seed(42)
    X = np.random.randn(n_samples, n_features)
    
    # æ·»åŠ æ—¶é—´ç›¸å…³ç‰¹å¾
    X[:, 0] = np.arange(n_samples) / n_samples  # è¶‹åŠ¿ç‰¹å¾
    X[:, 1] = np.sin(2 * np.pi * np.arange(n_samples) / 365.25)  # å¹´åº¦å­£èŠ‚æ€§
    if n_features > 2:
        X[:, 2] = np.cos(2 * np.pi * np.arange(n_samples) / 365.25)  # å¹´åº¦å­£èŠ‚æ€§
    if n_features > 3:
        X[:, 3] = np.sin(2 * np.pi * np.arange(n_samples) / 7)  # å‘¨å­£èŠ‚æ€§
    
    # ç”Ÿæˆç›®æ ‡å˜é‡
    y = np.zeros(n_samples)
    
    # æ·»åŠ è¶‹åŠ¿
    if trend:
        y += 2 * X[:, 0]
    
    # æ·»åŠ å­£èŠ‚æ€§
    if seasonality:
        y += 0.5 * X[:, 1]
        if n_features > 2:
            y += 0.3 * X[:, 2]
        if n_features > 3:
            y += 0.2 * X[:, 3]
    
    # æ·»åŠ ç‰¹å¾çš„çº¿æ€§ç»„åˆ
    y += 0.5 * X[:, -1]  # æœ€åä¸€ä¸ªç‰¹å¾
    
    # æ·»åŠ å™ªå£°
    y += noise_level * np.random.randn(n_samples)
    
    # è½¬æ¢ä¸ºDataFrame
    feature_names = [f'feature_{i}' for i in range(n_features)]
    X_df = pd.DataFrame(X, columns=feature_names, index=time_index)
    y_series = pd.Series(y, index=time_index, name='target')
    
    return X_df, y_series, time_index


def create_models() -> Dict[str, BaseEstimator]:
    """åˆ›å»ºæ¨¡å‹å­—å…¸"""
    models = {
        'linear_regression': LinearRegression(),
        'ridge': Ridge(alpha=1.0, random_state=42),
        'lasso': Lasso(alpha=0.1, random_state=42),
        'random_forest': RandomForestRegressor(
            n_estimators=100, 
            max_depth=10, 
            random_state=42,
            n_jobs=-1
        ),
        'gradient_boosting': GradientBoostingRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        ),
        'svr': SVR(kernel='rbf', C=1.0, gamma='scale'),
        'mlp': MLPRegressor(
            hidden_layer_sizes=(100, 50),
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
    }
    return models


def plot_data_splits(X: pd.DataFrame, y: pd.Series, splits_info: Dict[str, Any]):
    """å¯è§†åŒ–æ•°æ®åˆ’åˆ†"""
    fig, axes = plt.subplots(2, 1, figsize=(15, 10))
    
    # ç»˜åˆ¶åŸå§‹æ•°æ®
    axes[0].plot(X.index, y, label='Target Variable', alpha=0.7)
    axes[0].set_title('Time Series Data')
    axes[0].set_ylabel('Target Value')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶æ•°æ®åˆ’åˆ†
    n_samples = len(X)
    train_end = int(n_samples * 0.6)
    val_end = int(n_samples * 0.8)
    
    axes[1].plot(X.index[:train_end], y[:train_end], label='Training Set', color='blue')
    axes[1].plot(X.index[train_end:val_end], y[train_end:val_end], label='Validation Set', color='orange')
    axes[1].plot(X.index[val_end:], y[val_end:], label='Test Set', color='red')
    
    axes[1].axvline(x=X.index[train_end], color='blue', linestyle='--', alpha=0.7)
    axes[1].axvline(x=X.index[val_end], color='orange', linestyle='--', alpha=0.7)
    
    axes[1].set_title('Data Splits')
    axes[1].set_xlabel('Date')
    axes[1].set_ylabel('Target Value')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def plot_model_comparison(comparison_df: pd.DataFrame):
    """å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒç»“æœ"""
    # é€‰æ‹©ä¸»è¦æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒ
    metrics_to_plot = ['test_r2', 'test_mse', 'cv_mean_r2', 'wf_mean_score']
    available_metrics = [m for m in metrics_to_plot if m in comparison_df.columns]
    
    if not available_metrics:
        print("æ²¡æœ‰å¯ç”¨çš„æŒ‡æ ‡è¿›è¡Œå¯è§†åŒ–")
        return
    
    n_metrics = len(available_metrics)
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for i, metric in enumerate(available_metrics):
        if i >= 4:
            break
            
        ax = axes[i]
        data = comparison_df.set_index('model')[metric].sort_values(ascending=False)
        
        bars = ax.bar(range(len(data)), data.values)
        ax.set_xticks(range(len(data)))
        ax.set_xticklabels(data.index, rotation=45, ha='right')
        ax.set_title(f'{metric.replace("_", " ").title()}')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for j, bar in enumerate(bars):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(available_metrics), 4):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.show()


def plot_predictions(trainer: TimeSeriesTrainer, X_test: pd.DataFrame, y_test: pd.Series):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    best_model_name = trainer.get_best_model_name()
    predictions = trainer.predict(X_test.values, best_model_name)
    
    fig, axes = plt.subplots(2, 1, figsize=(15, 10))
    
    # é¢„æµ‹ vs å®é™…
    axes[0].plot(X_test.index, y_test, label='Actual', alpha=0.7)
    axes[0].plot(X_test.index, predictions, label=f'Predicted ({best_model_name})', alpha=0.7)
    axes[0].set_title('Predictions vs Actual (Test Set)')
    axes[0].set_ylabel('Target Value')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æ®‹å·®å›¾
    residuals = y_test - predictions
    axes[1].plot(X_test.index, residuals, alpha=0.7)
    axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.7)
    axes[1].set_title('Residuals')
    axes[1].set_xlabel('Date')
    axes[1].set_ylabel('Residual')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ—¶é—´åºåˆ—è®­ç»ƒç¤ºä¾‹")
    print("=" * 50)
    
    # 1. ç”Ÿæˆæ•°æ®
    print("ğŸ“Š ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®...")
    X, y, time_index = generate_time_series_data(
        n_samples=1000,
        n_features=5,
        trend=True,
        seasonality=True,
        noise_level=0.1
    )
    
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    print(f"æ—¶é—´èŒƒå›´: {time_index[0]} åˆ° {time_index[-1]}")
    
    # 2. é…ç½®è®­ç»ƒå‚æ•°
    print("\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    
    # æ—¶é—´åºåˆ—é…ç½®
    ts_config = TimeSeriesConfig(
        train_ratio=0.6,
        val_ratio=0.2,
        test_ratio=0.2,
        n_splits=5,
        validation_strategy='walk_forward',
        expanding_window=True,
        gap=0
    )
    
    # è®­ç»ƒé…ç½®
    training_config = TimeSeriesTrainingConfig(
        time_series_config=ts_config,
        use_time_series_cv=True,
        use_walk_forward=True,
        regression_metrics=['mse', 'mae', 'r2', 'rmse'],
        save_predictions=True,
        verbose=True
    )
    
    # 3. åˆ›å»ºæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºæ¨¡å‹...")
    models = create_models()
    print(f"åˆ›å»ºäº† {len(models)} ä¸ªæ¨¡å‹: {list(models.keys())}")
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    trainer = TimeSeriesTrainer(config=training_config)
    
    # æ·»åŠ æ¨¡å‹
    for name, model in models.items():
        trainer.add_model(name, model)
    
    # è®­ç»ƒ
    trainer.fit(X, y, time_index)
    
    # 5. è·å–ç»“æœ
    print("\nğŸ“ˆ åˆ†æç»“æœ...")
    comparison_df = trainer.get_model_comparison()
    print("\næ¨¡å‹æ¯”è¾ƒç»“æœ:")
    print(comparison_df.round(4))
    
    # 6. è·å–æœ€ä½³æ¨¡å‹
    best_model = trainer.get_best_model_name()
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model}")
    
    # 7. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    
    # æ•°æ®åˆ’åˆ†å¯è§†åŒ–
    plot_data_splits(X, y, trainer.data_splits['split_info'])
    
    # æ¨¡å‹æ¯”è¾ƒå¯è§†åŒ–
    plot_model_comparison(comparison_df)
    
    # é¢„æµ‹ç»“æœå¯è§†åŒ–
    X_test = trainer.data_splits['X_test']
    y_test = trainer.data_splits['y_test']
    
    # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿å¯è§†åŒ–
    test_index = time_index[int(len(time_index) * 0.8):]
    X_test_df = pd.DataFrame(X_test, columns=X.columns, index=test_index)
    y_test_series = pd.Series(y_test, index=test_index)
    
    plot_predictions(trainer, X_test_df, y_test_series)
    
    # 8. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    save_dir = "results/time_series_training"
    trainer.save_models(save_dir)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    os.makedirs(save_dir, exist_ok=True)
    comparison_df.to_csv(f"{save_dir}/detailed_comparison.csv", index=False)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    
    # 9. è¯¦ç»†åˆ†æ
    print("\nğŸ” è¯¦ç»†åˆ†æ:")
    print("-" * 30)
    
    for model_name, result in trainer.training_results.items():
        print(f"\næ¨¡å‹: {model_name}")
        print(f"  è®­ç»ƒæ—¶é—´: {result.training_time:.2f}ç§’")
        print(f"  æµ‹è¯•é›†RÂ²: {result.test_scores.get('r2', 'N/A'):.4f}")
        print(f"  æµ‹è¯•é›†RMSE: {result.test_scores.get('rmse', 'N/A'):.4f}")
        
        # äº¤å‰éªŒè¯ç»“æœ
        if model_name in trainer.cv_results:
            cv_r2 = trainer.cv_results[model_name]['mean_scores'].get('r2', 'N/A')
            cv_r2_std = trainer.cv_results[model_name]['std_scores'].get('r2', 'N/A')
            print(f"  CV RÂ²: {cv_r2:.4f} (Â±{cv_r2_std:.4f})")
        
        # Walk-forwardç»“æœ
        if model_name in trainer.walk_forward_results:
            wf_score = trainer.walk_forward_results[model_name]['mean_score']
            wf_std = trainer.walk_forward_results[model_name]['std_score']
            print(f"  Walk-forwardå¾—åˆ†: {wf_score:.4f} (Â±{wf_std:.4f})")
    
    print("\nâœ… æ—¶é—´åºåˆ—è®­ç»ƒç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    main()