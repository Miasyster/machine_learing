"""
æ•°æ®è·å–DAG
ä»Binance APIè·å–åŠ å¯†è´§å¸Kçº¿æ•°æ®
"""

import os
import sys
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.task_group import TaskGroup

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from operators.data_ingestion_operator import BinanceDataIngestionOperator, MultiSymbolDataIngestionOperator
from utils.task_helpers import (
    create_default_args, get_trading_pairs, get_default_intervals,
    get_data_path, get_task_timeout, get_retry_config
)


# DAGé…ç½®
DAG_ID = 'data_ingestion_dag'
DESCRIPTION = 'ä»Binance APIè·å–åŠ å¯†è´§å¸Kçº¿æ•°æ®'

# è·å–é…ç½®
default_args = create_default_args()
trading_pairs = get_trading_pairs()
intervals = get_default_intervals()
raw_data_path = get_data_path('raw')

# åˆ›å»ºDAG
dag = DAG(
    dag_id=DAG_ID,
    description=DESCRIPTION,
    default_args=default_args,
    schedule_interval='0 * * * *',  # æ¯å°æ—¶æ‰§è¡Œ
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['data', 'ingestion', 'binance', 'crypto']
)


def check_api_connection(**context):
    """æ£€æŸ¥Binance APIè¿æ¥"""
    import requests
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        # æµ‹è¯•Binance APIè¿æ¥
        response = requests.get('https://api.binance.com/api/v3/ping', timeout=10)
        response.raise_for_status()
        
        logger.info("Binance APIè¿æ¥æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.error(f"Binance APIè¿æ¥å¤±è´¥: {str(e)}")
        raise


def validate_raw_data(**context):
    """éªŒè¯åŸå§‹æ•°æ®"""
    import logging
    
    logger = logging.getLogger(__name__)
    
    # ä»XComè·å–æ•°æ®è·å–ç»“æœ
    task_instance = context['task_instance']
    
    # è·å–æ‰€æœ‰æ•°æ®è·å–ä»»åŠ¡çš„ç»“æœ
    results = {}
    for symbol in trading_pairs:
        for interval in intervals:
            task_id = f"fetch_data.{symbol.lower()}_{interval}_data"
            try:
                result = task_instance.xcom_pull(task_ids=task_id, key='ingestion_result')
                if result:
                    results[f"{symbol}_{interval}"] = result
            except Exception as e:
                logger.warning(f"æ— æ³•è·å– {task_id} çš„ç»“æœ: {str(e)}")
    
    # éªŒè¯ç»“æœ
    total_tasks = len(trading_pairs) * len(intervals)
    successful_tasks = len(results)
    success_rate = successful_tasks / total_tasks * 100
    
    logger.info(f"æ•°æ®è·å–æˆåŠŸç‡: {success_rate:.1f}% ({successful_tasks}/{total_tasks})")
    
    # æ£€æŸ¥æˆåŠŸç‡é˜ˆå€¼
    if success_rate < 80:
        logger.warning(f"æ•°æ®è·å–æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%")
    
    # è®¡ç®—æ€»æ•°æ®é‡
    total_data_points = sum(result.get('data_points', 0) for result in results.values())
    total_file_size = sum(result.get('file_size_mb', 0) for result in results.values())
    
    summary = {
        'total_tasks': total_tasks,
        'successful_tasks': successful_tasks,
        'success_rate': success_rate,
        'total_data_points': total_data_points,
        'total_file_size_mb': round(total_file_size, 2),
        'results': results
    }
    
    logger.info(f"æ•°æ®è·å–æ±‡æ€»: {total_data_points} æ¡æ•°æ®, {total_file_size:.2f} MB")
    
    # æ¨é€æ±‡æ€»ç»“æœåˆ°XCom
    task_instance.xcom_push(key='validation_summary', value=summary)
    
    return summary


def send_completion_notification(**context):
    """å‘é€å®Œæˆé€šçŸ¥"""
    import logging
    
    logger = logging.getLogger(__name__)
    
    # è·å–éªŒè¯ç»“æœ
    task_instance = context['task_instance']
    summary = task_instance.xcom_pull(task_ids='validate_raw_data', key='validation_summary')
    
    if summary:
        success_rate = summary.get('success_rate', 0)
        total_data_points = summary.get('total_data_points', 0)
        
        if success_rate >= 90:
            status = "âœ… æˆåŠŸ"
        elif success_rate >= 70:
            status = "âš ï¸ éƒ¨åˆ†æˆåŠŸ"
        else:
            status = "âŒ å¤±è´¥"
        
        message = f"""
æ•°æ®è·å–ä»»åŠ¡å®Œæˆ {status}

ğŸ“Š æ‰§è¡Œæ‘˜è¦:
- æˆåŠŸç‡: {success_rate:.1f}%
- æ•°æ®é‡: {total_data_points:,} æ¡
- æ–‡ä»¶å¤§å°: {summary.get('total_file_size_mb', 0):.2f} MB
- æ‰§è¡Œæ—¶é—´: {context['execution_date']}

ğŸ”— è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹Airflow Webç•Œé¢
        """
        
        logger.info(message)
        
        # TODO: å®ç°å®é™…çš„é€šçŸ¥å‘é€ï¼ˆé‚®ä»¶ã€Slackç­‰ï¼‰
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶æˆ–Slacké€šçŸ¥
        
    return "é€šçŸ¥å‘é€å®Œæˆ"


# å¼€å§‹ä»»åŠ¡
start_task = EmptyOperator(
    task_id='start',
    dag=dag
)

# APIè¿æ¥æ£€æŸ¥
api_check_task = PythonOperator(
    task_id='check_api_connection',
    python_callable=check_api_connection,
    dag=dag
)

# æ•°æ®è·å–ä»»åŠ¡ç»„
with TaskGroup('fetch_data', dag=dag) as fetch_data_group:
    
    for symbol in trading_pairs:
        for interval in intervals:
            # ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹å’Œæ—¶é—´é—´éš”åˆ›å»ºè·å–ä»»åŠ¡
            fetch_task = BinanceDataIngestionOperator(
                task_id=f'{symbol.lower()}_{interval}_data',
                symbol=symbol,
                interval=interval,
                limit=1000,
                connection_id='binance_api',
                storage_path=raw_data_path,
                file_format='csv',
                validate_data=True,
                **get_retry_config('api'),
                dag=dag
            )

# åŸå§‹æ•°æ®éªŒè¯
validate_task = PythonOperator(
    task_id='validate_raw_data',
    python_callable=validate_raw_data,
    dag=dag
)

# æ•°æ®ç›®å½•æ£€æŸ¥ï¼ˆç¡®ä¿æ•°æ®å·²ä¿å­˜ï¼‰
check_data_dir = BashOperator(
    task_id='check_data_directory',
    bash_command=f'ls -la {raw_data_path} && echo "æ•°æ®ç›®å½•æ£€æŸ¥å®Œæˆ"',
    dag=dag
)

# å®Œæˆé€šçŸ¥
notification_task = PythonOperator(
    task_id='send_completion_notification',
    python_callable=send_completion_notification,
    dag=dag
)

# ç»“æŸä»»åŠ¡
end_task = EmptyOperator(
    task_id='end',
    dag=dag
)

# å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
start_task >> api_check_task >> fetch_data_group >> validate_task >> check_data_dir >> notification_task >> end_task


# æ·»åŠ SLAç›‘æ§
def sla_miss_callback(dag, task_list, blocking_task_list, slas, blocking_tis):
    """SLAè¶…æ—¶å›è°ƒå‡½æ•°"""
    import logging
    
    logger = logging.getLogger(__name__)
    logger.error(f"SLAè¶…æ—¶: DAG={dag.dag_id}, ä»»åŠ¡={[task.task_id for task in task_list]}")
    
    # TODO: å‘é€SLAè¶…æ—¶å‘Šè­¦


# è®¾ç½®SLA
dag.sla_miss_callback = sla_miss_callback

# æ·»åŠ æ–‡æ¡£
dag.doc_md = """
# æ•°æ®è·å–DAG

## åŠŸèƒ½æè¿°
ä»Binance APIè·å–åŠ å¯†è´§å¸Kçº¿æ•°æ®ï¼Œæ”¯æŒå¤šä¸ªäº¤æ˜“å¯¹å’Œæ—¶é—´é—´éš”ã€‚

## æ‰§è¡Œæµç¨‹
1. **APIè¿æ¥æ£€æŸ¥**: éªŒè¯Binance APIå¯ç”¨æ€§
2. **æ•°æ®è·å–**: å¹¶è¡Œè·å–å¤šä¸ªäº¤æ˜“å¯¹çš„Kçº¿æ•°æ®
3. **æ•°æ®éªŒè¯**: éªŒè¯è·å–çš„æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§
4. **ç›®å½•æ£€æŸ¥**: ç¡®è®¤æ•°æ®æ–‡ä»¶å·²æ­£ç¡®ä¿å­˜
5. **å®Œæˆé€šçŸ¥**: å‘é€æ‰§è¡Œç»“æœé€šçŸ¥

## é…ç½®è¯´æ˜
- **è°ƒåº¦é¢‘ç‡**: æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
- **äº¤æ˜“å¯¹**: ä»é…ç½®æ–‡ä»¶æˆ–Airflowå˜é‡è·å–
- **æ—¶é—´é—´éš”**: æ”¯æŒ1hã€4hã€1dç­‰å¤šç§é—´éš”
- **æ•°æ®æ ¼å¼**: CSVæ ¼å¼å­˜å‚¨
- **å­˜å‚¨è·¯å¾„**: æŒ‰äº¤æ˜“å¯¹å’Œæ—¥æœŸåˆ†å±‚å­˜å‚¨

## ç›‘æ§æŒ‡æ ‡
- æ•°æ®è·å–æˆåŠŸç‡
- æ•°æ®é‡ç»Ÿè®¡
- æ–‡ä»¶å¤§å°ç»Ÿè®¡
- ä»»åŠ¡æ‰§è¡Œæ—¶é—´

## å‘Šè­¦æœºåˆ¶
- APIè¿æ¥å¤±è´¥å‘Šè­¦
- æ•°æ®è·å–æˆåŠŸç‡ä½äºé˜ˆå€¼å‘Šè­¦
- SLAè¶…æ—¶å‘Šè­¦
"""